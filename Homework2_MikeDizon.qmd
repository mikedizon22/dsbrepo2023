---
title: "Homerwork 2"
author: "MIKE DIZON"
date: 2023-05-21
format: 
  docx: default
  html:
    toc: true
    toc_float: true
    code-fold: true
editor: visual
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(wbstats)
library(skimr)
library(countrycode)
library(here)
```

# Data Visualisation - Exploration

Now that you've demonstrated your software is setup, and you have the basics of data manipulation, the goal of this assignment is to practice transforming, visualising, and exploring data.

# Mass shootings in the US

In July 2012, in the aftermath of a mass shooting in a movie theater in Aurora, Colorado, [Mother Jones](https://www.motherjones.com/politics/2012/07/mass-shootings-map/) published a report on mass shootings in the United States since 1982. Importantly, they provided the underlying data set as [an open-source database](https://www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/) for anyone interested in studying and understanding this criminal behavior.

## Obtain the data

```{r}
#| echo: false
#| message: false
#| warning: false


mass_shootings <- read_csv(here::here("data", "mass_shootings.csv"))

glimpse(mass_shootings)
```

| column(variable)     | description                                                                 |
|---------------------------|---------------------------------------------|
| case                 | short name of incident                                                      |
| year, month, day     | year, month, day in which the shooting occurred                             |
| location             | city and state where the shooting occcurred                                 |
| summary              | brief description of the incident                                           |
| fatalities           | Number of fatalities in the incident, excluding the shooter                 |
| injured              | Number of injured, non-fatal victims in the incident, excluding the shooter |
| total_victims        | number of total victims in the incident, excluding the shooter              |
| location_type        | generic location in which the shooting took place                           |
| male                 | logical value, indicating whether the shooter was male                      |
| age_of_shooter       | age of the shooter when the incident occured                                |
| race                 | race of the shooter                                                         |
| prior_mental_illness | did the shooter show evidence of mental illness prior to the incident?      |

## Explore the data

### Specific questions

-   Generate a data frame that summarizes the number of mass shootings per year.

```{r}

#Create a variable to store dataframe of number of mass shootings
number_mass_shootings <- mass_shootings %>% 

#Group by year
  group_by(year) %>% 
  
#Count the number of mass shootings per year
  summarize(number_mass_shootings = n())

```

-   Generate a bar chart that identifies the number of mass shooters associated with each race category. The bars should be sorted from highest to lowest and each bar should show its number.

```{r}

#Create a variable to store the dataframe of number of mass shooters per race
mass_shooters_per_race <- mass_shootings %>%
  
#Group by race
  group_by(race) %>%
  
#Count the number of mass shooters per race
  summarize(mass_shooters_per_race = n())

#Drop the NA values in dataframe
mass_shooters_per_race %>% 
  drop_na(race) %>% 

#Reorder the race based on the number of mass shooters per race
  mutate(race = fct_reorder(race,mass_shooters_per_race)) %>% 
  
#Create a plot of race vs number of mass shooters
  ggplot() +

#Set the race as x-axis and number of mass shooters y-axis
  aes(x = race, y = mass_shooters_per_race) +
  
#Create a bar graph
  geom_col() +
  
#Create data labels for each bar
  geom_text(aes(label = mass_shooters_per_race), vjust = -0.5, size = 3) +
  
#Create labels for the graph
  labs(title = "Number of Mass Shooters Per Race", x = "Race", y = "Number of Mass Shooters") +
  
#Set theme
  theme_minimal()

```

-   Generate a boxplot visualizing the number of total victims, by type of location.

```{r}

#Create a plot of total victims by location type
mass_shootings %>% 
  ggplot() +
  
#Set location type as x-axis and total victims as y-axis
  aes(x = location_type, y = total_victims) +
  
#Generate a boxplot
  geom_boxplot() +

#Create labels for the graph
  labs(title = "Number of Total Victims by Location type", x = "Location Type", y = "Total Victims") +
  
#Set theme
  theme_minimal()

```

-   Redraw the same plot, but remove the Las Vegas Strip massacre from the dataset.

```{r}

#Filter data to exclude the Las Vegas Strip massacre
mass_shootings %>% 
  filter(location != "Las Vegas, Nevada") %>% 
  
#Create the same boxplot as above
  ggplot() +
  
#Set location type as x-axis and total victims as y-axis
  aes(x = location_type, y = total_victims) +
  
#Generate a boxplot
  geom_boxplot() +
  
#Create labels for the graph
  labs(title = "Number of Total Victims by Location type", x = "Location Type", y = "Total Victims") +
  
#Set theme
  theme_minimal()
```

### More open-ended questions

Address the following questions. Generate appropriate figures/tables to support your conclusions.

-   How many white males with prior signs of mental illness initiated a mass shooting after 2000?

```{r}

#Filter the data based on set conditions to determine the number of white males with prior signs of mental illness after 2000
white_males_mental_illness_after_2000 <- mass_shootings %>% 
  filter(race == "White" & male == "TRUE" & prior_mental_illness == "Yes" & year>2000)

#Count the number
count(white_males_mental_illness_after_2000)

```

**Answer:** There were 22 white males with prior signs of mental illness who initiated a mass shooting after 2000.

-   Which month of the year has the most mass shootings? Generate a bar chart sorted in chronological (natural) order (Jan-Feb-Mar- etc) to provide evidence of your answer.

```{r}

#Create a variable to store the data frame with the month of the year with the most mass shootings
month_most_mass_shootings <- mass_shootings %>%
  
#Count the number of mass shootings per year
  count(month) %>% 
  
#Arrange in descending order
  arrange(desc(n))

#Create a variable that would indicate the correct order of months in a calendar year
month_order <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

#Create a plot of the number of mass shootings per month
month_most_mass_shootings %>% 

#Mutate so that the months will be in the desired order
  mutate(month = factor(month, levels = month_order, ordered = TRUE)) %>% 
  
#Use ggplot to create the diagram
  ggplot() +
  
#Set month as the x-axis and the number of mass shootings as the y-axis
  aes(x = month, y = n) +
  
#Create a bar graph
  geom_col() +
  
#Include labels for the graph
  labs(title = "Number of Mass Shootings Per Month", x = "Month", y = "Number of Mass Shootings") +

#Set theme
  theme_minimal()
```
**Answer:** As observed from the bar graph, February was the month with the most mass shootings at 13.

-   How does the distribution of mass shooting fatalities differ between White and Black shooters? What about White and Latino shooters?

```{r}

#Filter the data for mass shooting fatalities with Black shooters
black_shooters <- mass_shootings %>% 
  filter(race == "Black")

#Filter the data for mass shooting fatalities with White shooters
white_shooters <- mass_shootings %>% 
  filter(race == "White")

#Create a histogram to check the shape of the distribution of fatalities for Black and White shooters
ggplot() +
  
#Plot the first histogram for the mass shooting fatalities with Black shooters
  geom_histogram(
    data = black_shooters,

#Set the number of fatalities as the x-axis
    aes(x = fatalities), 

#Fill the color of the histogram bars with blue
    fill = "blue",

#Set transparency of histogram bars
    alpha = 0.5, 

#Set number of intervals into which data is divided
    bins = 30) +
  
#Plot the second histogram for the mass shooting fatalities with White shooters
  geom_histogram(
    data = white_shooters, 
    
#Set the number of fatalities as the x-axis
    aes(x=fatalities),

#Fill the color of the histogram bars with red
    fill = "red", 

#Set transparency of histogram bars
    alpha = 0.5, 

#Set number of intervals into which data is divided
    bins = 30) +
  
#Create labels for the histogram
  labs(title = "Distribution of Mass Shooting Fatalities by Shooter's Race", x = "Fatalities", y = "Count", fill = "Shooter's Race") +
  
#Manually specify the fill color for Black and White shooters
  scale_fill_manual(values = c("blue", "red"), labels = c("Black Shooter", "White Shooter")) +
  
#Set theme
  theme_minimal()
  
#Filter the data for mass shooting fatalities with Latino shooters
latino_shooters <- mass_shootings %>% 
  filter(race == "Latino")

#Create a histogram to check the shape of the distribution of fatalities for Latino and White shooters
ggplot() +
  
#Plot the first histogram for the mass shooting fatalities with White shooters
  geom_histogram(
    data = white_shooters,

#Set the number of fatalities as the x-axis
    aes(x = fatalities), 

#Fill the color of the histogram bars with red
    fill = "red",

#Set transparency of histogram bars
    alpha = 0.5, 

#Set number of intervals into which data is divided
    bins = 30) +
  
#Plot the second histogram for the mass shooting fatalities with Latino shooters
  geom_histogram(
    data = latino_shooters, 
    
#Set the number of fatalities as the x-axis
    aes(x=fatalities),

#Fill the color of the histogram bars with green
    fill = "green", 

#Set transparency of histogram bars
    alpha = 0.5, 

#Set number of intervals into which data is divided
    bins = 30) +
  
#Create labels for the histogram
  labs(title = "Distribution of Mass Shooting Fatalities by Shooter's Race", x = "Fatalities", y = "Count", fill = "Shooter's Race") +
  
#Manually specify the fill color for Black and White shooters
  scale_fill_manual(values = c("red", "green"), labels = c("White Shooter", "Latino Shooter")) +
  
#Set theme
  theme_minimal()
```

**Answer:** The shape of the distribution for the number of fatalities between Black and White shooters are similar in a sense that both are skewed to the right. However, they differ because there are more instances and extreme values for the number of fatalities with White shooters as opposed to Black shooters.

Similarly, the shape of the distribution for the number of fatalities between White and Latino shooters are almost the same with the previous comparison. That is, both are skewed to the right but there are more instances and extreme values for the number of fatalities with White shooters as opposed to Latino shooters.

### Very open-ended

-   Are mass shootings with shooters suffering from mental illness different from mass shootings with no signs of mental illness in the shooter?

```{r}

#Filter the data for only those shooters suffering from prior mental illness
with_mental_illness <- mass_shootings %>% 
  filter(prior_mental_illness == "Yes")

#Filter the data for only those shooters not suffering from prior mental illness 
no_mental_illness <- mass_shootings %>% 
  filter(prior_mental_illness == "No")

#Combine the two datasets
combined_data <- rbind(with_mental_illness, no_mental_illness)

#Create a grouped bar chart using ggplot
combined_data %>% 
ggplot() +

#Set x-axis as the presence of mental illness and set the same as the fill
  aes(x = prior_mental_illness, fill = prior_mental_illness) +

#Create a bar graph
  geom_bar() +
    
#Create labels for the graph
  labs(x = "Prior Mental Illness", y = "Number of Shootings", title = "Comparison of Mass Shootings\nwith Mental Illness vs. No Mental Illness") +
    
#Set theme
  theme_minimal()

```

**Answer:** The graph above shows that there are more mass shooting fatalities if mass shooters had evidence of prior mental illness. We can therefore conclude that it is more likely for mass shooters in the sample to engage in mass shootings if they are mentally sick.

-   Assess the relationship between mental illness and total victims, mental illness and location type, and the intersection of all three variables.

```{r}

#Filter out the NA values for prior mental illness from the data frame
mass_shootings %>%
  filter(!is.na(prior_mental_illness)) %>%
  
#Use ggplot to assess relationship between mental illness and total victims
ggplot() +
  
#Set prior mental illness as x-axis and the number of total victims as y-axis
  aes(x = prior_mental_illness, y = total_victims) +
  
#Create a scatter plot
  geom_col() +
  
#Create labels for the graph
  labs(x = "Prior Mental Illness", y = "Total Victims") +
  ggtitle("Relationship between Mental Illness and Total Victims") +
  
#Set theme
  theme_minimal()

#Filter out the NA values for prior mental illness from the data frame
mass_shootings %>%
  filter(!is.na(prior_mental_illness)) %>%

#Use ggplot to assess relationship between mental illness and location type
ggplot() +
  
#Set prior mental illness as x-axis and location type as y-axis, and use location type as fill
  aes(x = prior_mental_illness, fill = location_type) +
  
#Create bar graph
  geom_bar() +
  
#Create labels for the graph
  labs(x = "Prior Mental Illness", y = "Count") +
  ggtitle("Comparison of Location Types based on Mental Illness") +
  
#Set theme
  theme_minimal()

#Filter out the NA values for prior mental illness from the data frame
mass_shootings %>% 
  filter(!is.na(prior_mental_illness)) %>% 
  
#Use ggplot to assess relationship across the three variables
  ggplot() +
  
#Set prior mental illness as x-axis, total victims as y-axis, and location type as fill
  aes(x = prior_mental_illness, y = total_victims, fill = location_type) +
  
#Create a bar graph and set the height and positioning of the bars
  geom_bar(stat = "identity", position = "dodge") +
  
#Create labels for the graph
  labs(x = "Prior Mental Illness", y = "Total Victims") +
  ggtitle("Comparison of Mental Illness, Location Type, and Total Victims") +
  
#Set theme
  theme_minimal()

```

**Answer:** As seen from the first graph, the total number of victims is significantly higher for shooters with prior mental illness compared with those who do not.Further expanding this analysis through the second graph shows that mass shooters with no prior mental illness conduct most mass shootings in other location types or in the workplace, which is also similar to those with prior mental illness.The third graph shows that the number of total victims are highest in both other location types for mass shooters with no prior mental illness and for those who have.

# Exploring credit card fraud

We will be using a dataset with credit card transactions containing legitimate and fraud transactions. Fraud is typically well below 1% of all transactions, so a naive model that predicts that all transactions are legitimate and not fraudulent would have an accuracy of well over 99%-- pretty good, no? (well, not quite as we will see later in the course)

You can read more on credit card fraud on [Credit Card Fraud Detection Using Weighted Support Vector Machine](https://www.scirp.org/journal/paperinformation.aspx?paperid=105944)

The dataset we will use consists of credit card transactions and it includes information about each transaction including customer details, the merchant and category of purchase, and whether or not the transaction was a fraud.

## Obtain the data

The dataset is too large to be hosted on Canvas or Github, so please download it from dropbox https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0 and save it in your `dsb` repo, under the `data` folder

```{r}
#| echo: false
#| message: false
#| warning: false

card_fraud <- read_csv(here::here("data", "card_fraud.csv"))

glimpse(card_fraud)
```

The data dictionary is as follows

| column(variable)      | description                                 |
|-----------------------|---------------------------------------------|
| trans_date_trans_time | Transaction DateTime                        |
| trans_year            | Transaction year                            |
| category              | category of merchant                        |
| amt                   | amount of transaction                       |
| city                  | City of card holder                         |
| state                 | State of card holder                        |
| lat                   | Latitude location of purchase               |
| long                  | Longitude location of purchase              |
| city_pop              | card holder's city population               |
| job                   | job of card holder                          |
| dob                   | date of birth of card holder                |
| merch_lat             | Latitude Location of Merchant               |
| merch_long            | Longitude Location of Merchant              |
| is_fraud              | Whether Transaction is Fraud (1) or Not (0) |

-   In this dataset, how likely are fraudulent transactions? Generate a table that summarizes the number and frequency of fraudulent transactions per year.

```{r}

# Calculate the number and frequency of fraudulent transactions per year
fraud_summary <- card_fraud %>%
  group_by(trans_year) %>%
  summarize(num_fraud_transactions = sum(is_fraud),
            frequency = num_fraud_transactions / n())

```

-   How much money (in US\$ terms) are fraudulent transactions costing the company? Generate a table that summarizes the total amount of legitimate and fraudulent transactions per year and calculate the % of fraudulent transactions, in US\$ terms.

```{r}

# Calculate the total amount of legitimate and fraudulent transactions per year
transaction_summary <- card_fraud %>%
  group_by(trans_year) %>%
  summarize(total_legitimate_amount = sum(amt * (1 - is_fraud)),
            total_fraudulent_amount = sum(amt * is_fraud))

# Calculate the percentage of fraudulent transactions in US$ terms
transaction_summary <- transaction_summary %>%
  mutate(fraud_percentage = total_fraudulent_amount / (total_legitimate_amount + total_fraudulent_amount) * 100)

```

-   Generate a histogram that shows the distribution of amounts charged to credit card, both for legitimate and fraudulent accounts. Also, for both types of transactions, calculate some quick summary statistics.

```{r}

# Filter the data for legitimate transactions
legitimate_transactions <- card_fraud %>%
  filter(is_fraud == 0)

# Filter the data for fraudulent transactions
fraudulent_transactions <- card_fraud %>%
  filter(is_fraud == 1)

# Create a histogram for legitimate transactions
ggplot(legitimate_transactions, aes(x = amt)) +
  geom_histogram() +
  labs(x = "Amount", y = "Frequency", title = "Distribution of Amounts - Legitimate Transactions")

# Calculate summary statistics for legitimate transactions
legitimate_summary <- legitimate_transactions %>%
  summarize(
    mean_amount = mean(amt),
    median_amount = median(amt),
    min_amount = min(amt),
    max_amount = max(amt)
  )

# Create a histogram for fraudulent transactions
ggplot(fraudulent_transactions, aes(x = amt)) +
  geom_histogram() +
  labs(x = "Amount", y = "Frequency", title = "Distribution of Amounts - Fraudulent Transactions")

# Calculate summary statistics for fraudulent transactions
fraudulent_summary <- fraudulent_transactions %>%
  summarize(
    mean_amount = mean(amt),
    median_amount = median(amt),
    min_amount = min(amt),
    max_amount = max(amt)
  )

```

-   What types of purchases are most likely to be instances of fraud? Consider category of merchants and produce a bar chart that shows % of total fraudulent transactions sorted in order.

```{r}

# Calculate the percentage of total fraudulent transactions per merchant category
fraud_category <- card_fraud %>%
  group_by(category) %>%
  summarize(total_fraud_transactions = sum(is_fraud)) %>%
  mutate(percentage = (total_fraud_transactions / sum(total_fraud_transactions)) * 100) %>%
  arrange(desc(percentage))

# Create a bar chart
ggplot(fraud_category, aes(x = reorder(category, -percentage), y = percentage, fill = category)) +
  geom_bar(stat = "identity") +
  labs(x = "Merchant Category", y = "% of Total Fraudulent Transactions", 
       title = "Percentage of Total Fraudulent Transactions by Merchant Category") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

-   When is fraud more prevalent? Which days, months, hours? To create new variables to help you in your analysis, we use the `lubridate` package and the following code

```         
mutate(
  date_only = lubridate::date(trans_date_trans_time),
  month_name = lubridate::month(trans_date_trans_time, label=TRUE),
  hour = lubridate::hour(trans_date_trans_time),
  weekday = lubridate::wday(trans_date_trans_time, label = TRUE)
  )
```

-   Are older customers significantly more likely to be victims of credit card fraud? To calculate a customer's age, we use the `lubridate` package and the following code

```         
  mutate(
   age = interval(dob, trans_date_trans_time) / years(1),
    )
```

```{r}

# Create new variables using the lubridate package
card_fraud <- card_fraud %>%
  mutate(
    date_only = date(trans_date_trans_time),
    month_name = month(trans_date_trans_time, label = TRUE),
    hour = hour(trans_date_trans_time),
    weekday = wday(trans_date_trans_time, label = TRUE)
  )

# Analyze fraud prevalence by day
fraud_by_day <- card_fraud %>%
  group_by(date_only) %>%
  summarize(total_fraud_transactions = sum(is_fraud))

# Analyze fraud prevalence by month
fraud_by_month <- card_fraud %>%
  group_by(month_name) %>%
  summarize(total_fraud_transactions = sum(is_fraud))

# Analyze fraud prevalence by hour
fraud_by_hour <- card_fraud %>%
  group_by(hour) %>%
  summarize(total_fraud_transactions = sum(is_fraud))

# Create plots to visualize fraud prevalence
ggplot(fraud_by_day, aes(x = date_only, y = total_fraud_transactions)) +
  geom_line() +
  labs(x = "Date", y = "Total Fraudulent Transactions", title = "Fraud Prevalence by Day")

ggplot(fraud_by_month, aes(x = month_name, y = total_fraud_transactions)) +
  geom_bar(stat = "identity") +
  labs(x = "Month", y = "Total Fraudulent Transactions", title = "Fraud Prevalence by Month")

ggplot(fraud_by_hour, aes(x = hour, y = total_fraud_transactions)) +
  geom_bar(stat = "identity") +
  labs(x = "Hour", y = "Total Fraudulent Transactions", title = "Fraud Prevalence by Hour")


# Calculate customer age at the time of the transaction
card_fraud <- card_fraud %>%
  mutate(
    age = interval(dob, trans_date_trans_time) / years(1)
  )

# Compare the age distribution between fraudulent and non-fraudulent transactions
ggplot(card_fraud, aes(x = age, fill = is_fraud)) +
  geom_histogram(binwidth = 5, position = "identity", alpha = 0.7) +
  labs(x = "Age", y = "Count", title = "Age Distribution: Fraudulent vs Non-Fraudulent Transactions",
       fill = "Fraudulent") +
  scale_fill_manual(values = c("#2C77BF", "#E24A33")) +
  theme_minimal()

```

-   Is fraud related to distance? The distance between a card holder's home and the location of the transaction can be a feature that is related to fraud. To calculate distance, we need the latidue/longitude of card holders's home and the latitude/longitude of the transaction, and we will use the [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula) to calculate distance. I adapted code to [calculate distance between two points on earth](https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/) which you can find below

```{r}
# distance between card holder's home and transaction
# code adapted from https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/


card_fraud <- card_fraud %>%
  mutate(
    
    # convert latitude/longitude to radians
    lat1_radians = lat / 57.29577951,
    lat2_radians = merch_lat / 57.29577951,
    long1_radians = long / 57.29577951,
    long2_radians = merch_long / 57.29577951,
    
    # calculate distance in miles
    distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)),

    # calculate distance in km
    distance_km = 6377.830272 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians))

  )

# Summary statistics
summary_stats <- card_fraud %>%
  summarize(
    mean_distance = mean(distance_km),
    median_distance = median(distance_km),
    min_distance = min(distance_km),
    max_distance = max(distance_km),
    q1_distance = quantile(distance_km, 0.25),
    q3_distance = quantile(distance_km, 0.75)
  )

# Boxplot
ggplot(card_fraud, aes(x = is_fraud, y = distance_km)) +
  geom_boxplot() +
  labs(x = "Fraudulent", y = "Distance (km)") +
  ggtitle("Distribution of Distance for Fraudulent and Non-Fraudulent Transactions")

# Density plot
ggplot(card_fraud, aes(x = distance_km, fill = is_fraud)) +
  geom_density(alpha = 0.5) +
  labs(x = "Distance (km)", y = "Density") +
  ggtitle("Density Distribution of Distance for Fraudulent and Non-Fraudulent Transactions") +
  scale_fill_manual(values = c("#FF0000", "#0000FF"), labels = c("Non-Fraudulent", "Fraudulent"))

```

Plot a boxplot or a violin plot that looks at the relationship of distance and `is_fraud`. Does distance seem to be a useful feature in explaining fraud?

```{r}


# Boxplot
ggplot(card_fraud, aes(x = is_fraud, y = distance_km)) +
  geom_boxplot() +
  labs(x = "Fraudulent", y = "Distance (km)") +
  ggtitle("Relationship between Distance and Fraud") +
  theme_minimal()

# Violin plot
ggplot(card_fraud, aes(x = is_fraud, y = distance_km)) +
  geom_violin() +
  labs(x = "Fraudulent", y = "Distance (km)") +
  ggtitle("Relationship between Distance and Fraud") +
  theme_minimal()


```

# Exploring sources of electricity production, CO2 emissions, and GDP per capita.

There are many sources of data on how countries generate their electricity and their CO2 emissions. I would like you to create three graphs:

## 1. A stacked area chart that shows how your own country generated its electricity since 2000.

You will use

`geom_area(colour="grey90", alpha = 0.5, position = "fill")`

```{r}

# Filter the energy dataset for the Philippines and select the relevant columns
philippines_energy <- energy %>% 
  filter(iso_code == "PHL") %>% 
  select(year, coal, gas, hydro, nuclear, oil, other_renewable, solar, wind)

# Reshape the data from wide to long format
philippines_energy_long <- philippines_energy %>% 
  pivot_longer(cols = -year, names_to = "source", values_to = "electricity_generation")

# Create the stacked area chart
ggplot(philippines_energy_long, aes(x = year, y = electricity_generation, fill = source)) +
  geom_area(colour = "grey90", alpha = 0.5, position = "fill") +
  labs(x = "Year", y = "Electricity Generation", fill = "Source") +
  scale_fill_manual(values = c("coal" = "darkred", "gas" = "orange", "hydro" = "blue", 
                               "nuclear" = "darkgreen", "oil" = "brown", "other_renewable" = "green", 
                               "solar" = "yellow", "wind" = "lightblue")) +
  ggtitle("Electricity Generation in the Philippines since 2000") +
  theme_minimal()

```

## 2. A scatter plot that looks at how CO2 per capita and GDP per capita are related

```{r}

# Filter data for the Philippines
co2_gdp_ph <- co2_gdp %>% filter(country.x == "Philippines")

# Create scatter plot for CO2 per capita and GDP per capita
plot_co2_gdp_ph <- ggplot(co2_gdp_ph, aes(x = GDPpercap, y = co2percap)) +
  geom_point() +
  geom_text(aes(label = year), vjust = -0.5, hjust = 0.5, size = 3) +
  labs(x = "GDP per Capita", y = "CO2 per Capita") +
  ggtitle("Relationship between CO2 per Capita and GDP per Capita (Philippines)") +
  theme_minimal()

# Display the scatter plot
plot_co2_gdp_ph

```

## 3. A scatter plot that looks at how electricity usage (kWh) per capita/day GDP per capita are related

```{r}

# Filter data for the Philippines
energy_gdp_ph <- energy_gdp %>% filter(country.x == "Philippines")

# Create scatter plot for electricity usage per capita and GDP per capita
plot_energy_gdp_ph <- ggplot(energy_gdp_ph, aes(x = GDPpercap, y = electricity_demand)) +
  geom_point() +
  geom_text(aes(label = year), vjust = -0.5, hjust = 0.5, size = 3) +
  labs(x = "GDP per Capita", y = "Electricity Usage (kWh per Capita/Day)") +
  ggtitle("Relationship between Electricity Usage and GDP per Capita (Philippines)") +
  theme_minimal()

# Display the scatter plot
plot_energy_gdp_ph

```

We will get energy data from the Our World in Data website, and CO2 and GDP per capita emissions from the World Bank, using the `wbstats`package.

```{r}
#| message: false
#| warning: false

# Download electricity data
url <- "https://nyc3.digitaloceanspaces.com/owid-public/data/energy/owid-energy-data.csv"

energy <- read_csv(url) %>% 
  filter(year >= 1990) %>% 
  drop_na(iso_code) %>% 
  select(1:3,
         biofuel = biofuel_electricity,
         coal = coal_electricity,
         gas = gas_electricity,
         hydro = hydro_electricity,
         nuclear = nuclear_electricity,
         oil = oil_electricity,
         other_renewable = other_renewable_exc_biofuel_electricity,
         solar = solar_electricity,
         wind = wind_electricity, 
         electricity_demand,
         electricity_generation,
         net_elec_imports,	# Net electricity imports, measured in terawatt-hours
         energy_per_capita,	# Primary energy consumption per capita, measured in kilowatt-hours	Calculated by Our World in Data based on BP Statistical Review of World Energy and EIA International Energy Data
         energy_per_gdp,	# Energy consumption per unit of GDP. This is measured in kilowatt-hours per 2011 international-$.
         per_capita_electricity, #	Electricity generation per capita, measured in kilowatt-hours
  ) 

# Download data for C02 emissions per capita https://data.worldbank.org/indicator/EN.ATM.CO2E.PC
co2_percap <- wb_data(country = "countries_only", 
                      indicator = "EN.ATM.CO2E.PC", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         co2percap = value)


# Download data for GDP per capita  https://data.worldbank.org/indicator/NY.GDP.PCAP.PP.KD
gdp_percap <- wb_data(country = "countries_only", 
                      indicator = "NY.GDP.PCAP.PP.KD", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         GDPpercap = value)
```

Specific questions:

1.  How would you turn `energy` to long, tidy format?

```{r}

energy_tidy <- energy %>%
  pivot_longer(cols = starts_with(c("biofuel", "coal", "gas", "hydro", "nuclear", "oil", "other_renewable", "solar", "wind")),
               names_to = "energy_source",
               values_to = "electricity_generated")
```

2.  You may need to join these data frames
    -   Use `left_join` from `dplyr` to [join the tables](http://r4ds.had.co.nz/relational-data.html)
    -   To complete the merge, you need a unique *key* to match observations between the data frames. Country names may not be consistent among the three dataframes, so please use the 3-digit ISO code for each country
    -   An aside: There is a great package called [`countrycode`](https://github.com/vincentarelbundock/countrycode) that helps solve the problem of inconsistent country names (Is it UK? United Kingdon? Great Britain?). `countrycode()` takes as an input a country's name in a specific format and outputs it using whatever format you specify.
    
```{r}

# Join the data frames using ISO code as the key
merged_data <- left_join(co2_percap, gdp_percap, by = c("iso2c", "year"))

```


3.  Write a function that takes as input any country's name and returns all three graphs. You can use the `patchwork` package to arrange the three graphs as shown below

```{r}

generate_country_graphs <- function(country_name) {
  # Graph 1: Stacked Area Chart - Electricity Generation
  philippines_energy <- energy %>% 
    filter(iso_code == countrycode::countrycode(country_name, "country.name", "iso3c")) %>% 
    select(year, coal, gas, hydro, nuclear, oil, other_renewable, solar, wind)
  
  philippines_energy_long <- philippines_energy %>% 
    pivot_longer(cols = -year, names_to = "source", values_to = "electricity_generation")
  
  plot_energy_ph <- ggplot(philippines_energy_long, aes(x = year, y = electricity_generation, fill = source)) +
    geom_area(colour = "grey90", alpha = 0.5, position = "fill") +
    labs(x = "Year", y = "Electricity Generation", fill = "Source") +
    scale_fill_manual(values = c("coal" = "darkred", "gas" = "orange", "hydro" = "blue", 
                                 "nuclear" = "darkgreen", "oil" = "brown", "other_renewable" = "green", 
                                 "solar" = "yellow", "wind" = "lightblue")) +
    ggtitle(paste("Electricity Generation in", country_name, "since 2000")) +
    theme_minimal()
  
  # Graph 2: Scatter Plot - CO2 per Capita vs. GDP per Capita
  co2_gdp_ph <- co2_gdp %>% 
    filter(country.x == country_name)
  
  plot_co2_gdp_ph <- ggplot(co2_gdp_ph, aes(x = GDPpercap, y = co2percap)) +
    geom_point() +
    geom_text(aes(label = year), vjust = -0.5, hjust = 0.5, size = 3) +
    labs(x = "GDP per Capita", y = "CO2 per Capita") +
    ggtitle(paste("Relationship between CO2 per Capita and GDP per Capita (", country_name, ")")) +
    theme_minimal()
  
  # Graph 3: Scatter Plot - Electricity Usage per Capita vs. GDP per Capita
  energy_gdp_ph <- energy_gdp %>% 
    filter(country.x == country_name)
  
  plot_energy_gdp_ph <- ggplot(energy_gdp_ph, aes(x = GDPpercap, y = electricity_demand)) +
    geom_point() +
    geom_text(aes(label = year), vjust = -0.5, hjust = 0.5, size = 3) +
    labs(x = "GDP per Capita", y = "Electricity Usage (kWh per Capita/Day)") +
    ggtitle(paste("Relationship between Electricity Usage and GDP per Capita (", country_name, ")")) +
    theme_minimal()
  
  # Arrange the graphs using patchwork
  all_graphs <- plot_energy_ph + plot_co2_gdp_ph + plot_energy_gdp_ph
  all_graphs <- all_graphs + plot_layout(ncol = 1)
  
  # Return the arranged graphs
  return(all_graphs)
}

# Usage example: Generate graphs for the Philippines
generate_country_graphs("Philippines")


```


![](images/electricity-co2-gdp.png)

# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed Quarto Markdown (qmd) file as a Word document (use the "Render" button at the top of the script editor window) and upload it to Canvas. You must be commiting and pushing tour changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: TYPE NAMES HERE
-   Approximately how much time did you spend on this problem set: ANSWER HERE
-   What, if anything, gave you the most trouble: ANSWER HERE

**Please seek out help when you need it,** and remember the [15-minute rule](https://mam2022.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
